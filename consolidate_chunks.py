#!/usr/bin/env python3
"""
Chunk Consolidation Script

Reads all chunks generated by regulation_filter.py and creates a full JSON
in the schema of consolidate_regulations.py from all chunks of each country.
Saves consolidated files in output/{country_name}/
"""

import json
import os
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
import glob

# Configuration
INPUT_DIR = "regulation_analysis_results"
OUTPUT_DIR = "output"

class ConsolidatedRegulation(BaseModel):
    """Pydantic model for the final consolidated regulation"""
    unique_id: str = Field(description="Unique identifier for the consolidated regulation")
    country: str = Field(description="Primary country or 'Multiple' if cross-jurisdictional")
    jurisdiction: str = Field(description="Primary jurisdiction or summary of multiple")
    issuing_body: str = Field(description="Primary authority or summary of multiple bodies")
    tag: str = Field(description="Primary disclosure area or summary of areas covered")
    regulation_name: str = Field(description="Comprehensive name covering all sub-regulations")
    publication_date: str = Field(description="Most recent or range of publication dates")
    regulation_status: str = Field(description="Overall status across all regulations")
    summary: str = Field(description="Comprehensive summary of all disclosure requirements")
    applicability: str = Field(description="Combined scope summary covering all regulations")
    scoping_threshold: str = Field(description="Combined minimum thresholds across regulations")
    effective_date: str = Field(description="Earliest or most relevant mandatory reporting date")
    timeline_details: str = Field(description="Comprehensive implementation timeline")
    financial_integration: str = Field(description="Overall integration with financial reporting")
    filing_mechanism: str = Field(description="Summary of all filing mechanisms")
    reporting_frequency: str = Field(description="Summary of required reporting frequencies")
    assurance_requirement: str = Field(description="Overall assurance requirements")
    penalties: str = Field(description="Summary of all non-compliance penalties")
    full_text_link: str = Field(description="Primary or most comprehensive regulation link")
    translated_flag: bool = Field(description="Whether any content was translated")
    source_url: str = Field(description="Primary source or summary of sources")
    last_scraped: str = Field(description="Most recent scraping date")
    change_detected: bool = Field(description="Whether any changes were detected")


class ChunkConsolidator:
    def __init__(self):
        self.llm = ChatOpenAI(
            api_key=os.getenv("OPENAI_API_KEY", "your-api-key-here"),
            base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1"),
            temperature=0.1,
            model="gpt-4o-mini"
        )
        
        self.parser = JsonOutputParser(pydantic_object=ConsolidatedRegulation)
        
        self.prompt = ChatPromptTemplate.from_template("""
You are an expert regulatory analyst. Analyze the following regulation document chunks from the same country and create ONE comprehensive consolidated regulation that summarizes all the regulations.

REGULATION CHUNKS TO CONSOLIDATE:
{regulations_text}

COUNTRY: {country}

INSTRUCTIONS:
1. Create a SINGLE consolidated regulation that represents the combined regulatory landscape for {country}
2. Merge similar requirements across regulations from the same country
3. Identify the primary jurisdiction, authority, and scope for {country}
4. Create comprehensive summaries that capture all key requirements
5. For dates, use the most relevant or create ranges
6. For IDs, create a new consolidated ID in format: {country}_CONSOLIDATED_{year}
7. Ensure the consolidated regulation captures the essence of all regulations from {country}
8. If multiple jurisdictions within the country, create summary descriptions
9. Prioritize the most recent and comprehensive regulations
10. Include all relevant disclosure areas and requirements

{format_instructions}
""")
        
        self.chain = self.prompt | self.llm | self.parser

    def consolidate_country_regulations(self, country: str, regulations: List[Dict]) -> Dict:
        """Use LLM to consolidate multiple regulations from same country into one comprehensive regulation"""
        
        # Prepare regulations text for LLM
        regulations_text = ""
        for i, reg in enumerate(regulations, 1):
            regulations_text += f"\n--- REGULATION CHUNK {i} ---\n"
            regulations_text += f"Name: {reg.get('regulation_name', 'Unknown')}\n"
            regulations_text += f"Authority: {reg.get('issuing_body', 'Unknown')}\n"
            regulations_text += f"Tag: {reg.get('tag', 'Unknown')}\n"
            regulations_text += f"Summary: {reg.get('summary', 'Unknown')}\n"
            regulations_text += f"Applicability: {reg.get('applicability', 'Unknown')}\n"
            regulations_text += f"Effective Date: {reg.get('effective_date', 'Unknown')}\n"
            regulations_text += f"Penalties: {reg.get('penalties', 'Unknown')}\n"
            regulations_text += f"Filing Mechanism: {reg.get('filing_mechanism', 'Unknown')}\n"
            regulations_text += f"Reporting Frequency: {reg.get('reporting_frequency', 'Unknown')}\n"
            regulations_text += f"Financial Integration: {reg.get('financial_integration', 'Unknown')}\n"
            regulations_text += f"Full Details: {json.dumps(reg, indent=2)}\n"
        
        # Use LLM to create consolidated regulation
        try:
            result = self.chain.invoke({
                "regulations_text": regulations_text,
                "country": country,
                "year": datetime.now().year,
                "format_instructions": self.parser.get_format_instructions()
            })
            return result
            
        except Exception as e:
            print(f"Error in LLM consolidation for {country}: {e}")
            # Fallback to manual consolidation
            return self.manual_consolidation_fallback(country, regulations)
    
    def manual_consolidation_fallback(self, country: str, regulations: List[Dict]) -> Dict:
        """Fallback manual consolidation if LLM fails"""
        authorities = list(set(r.get('issuing_body', 'Unknown') for r in regulations if r.get('issuing_body') != 'Unknown'))
        tags = list(set(r.get('tag', 'Unknown') for r in regulations if r.get('tag') != 'Unknown'))
        
        return {
            "unique_id": f"{country}_CONSOLIDATED_{datetime.now().year}",
            "country": country,
            "jurisdiction": f"{country} National",
            "issuing_body": "; ".join(authorities[:3]) + ("..." if len(authorities) > 3 else ""),
            "tag": "; ".join(tags[:3]) + ("..." if len(tags) > 3 else ""),
            "regulation_name": f"{country} Consolidated Regulatory Framework ({len(regulations)} regulations)",
            "publication_date": f"{datetime.now().year}",
            "regulation_status": "Consolidated",
            "summary": f"Comprehensive regulatory framework for {country} covering {len(regulations)} related regulations across {len(set(r.get('tag', 'Unknown') for r in regulations))} disclosure areas",
            "applicability": f"Entities operating in {country} subject to various regulatory requirements",
            "scoping_threshold": "Various thresholds apply based on entity type and size",
            "effective_date": f"{datetime.now().year}",
            "timeline_details": "Multiple implementation phases and compliance deadlines",
            "financial_integration": "Yes - integrated with financial reporting requirements",
            "filing_mechanism": "Multiple filing mechanisms depending on regulation type",
            "reporting_frequency": "Various frequencies from annual to quarterly reporting",
            "assurance_requirement": "Various assurance requirements depending on regulation",
            "penalties": "Multiple penalty frameworks for non-compliance",
            "full_text_link": regulations[0].get('full_text_link', 'Unknown') if regulations else 'Unknown',
            "translated_flag": any(r.get('translated_flag', False) for r in regulations),
            "source_url": f"Multiple sources consolidated for {country}",
            "last_scraped": datetime.now().strftime("%Y-%m-%d"),
            "change_detected": False
        }


def find_all_chunks(input_dir: str) -> Dict[str, List[str]]:
    """Find all chunk files organized by country"""
    input_path = Path(input_dir)
    
    if not input_path.exists():
        print(f"Error: Input directory not found: {input_path}")
        return {}
    
    country_chunks = {}
    
    # Find all country directories
    for country_dir in input_path.iterdir():
        if not country_dir.is_dir():
            continue
            
        country = country_dir.name
        chunk_files = []
        
        # Find all JSON files in country directory and subdirectories
        for json_file in country_dir.rglob("*.json"):
            # Skip summary files, only process individual analysis files
            if "summary" not in json_file.name.lower():
                chunk_files.append(str(json_file))
        
        if chunk_files:
            country_chunks[country] = chunk_files
            print(f"Found {len(chunk_files)} chunks for {country}")
    
    return country_chunks


def load_chunk_data(chunk_file: str) -> Dict[str, Any]:
    """Load data from a chunk file"""
    try:
        with open(chunk_file, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading chunk {chunk_file}: {e}")
        return None


def consolidate_country_chunks(country: str, chunk_files: List[str], consolidator: ChunkConsolidator) -> Dict[str, Any]:
    """Consolidate all chunks for a specific country"""
    print(f"\nðŸ”„ Consolidating {len(chunk_files)} chunks for {country}")
    
    # Load all chunk data
    regulations = []
    valid_chunks = 0
    
    for chunk_file in chunk_files:
        chunk_data = load_chunk_data(chunk_file)
        if chunk_data and chunk_data.get('relevant', False):
            regulations.append(chunk_data)
            valid_chunks += 1
    
    if not regulations:
        print(f"  No relevant regulations found for {country}")
        return None
    
    print(f"  Found {valid_chunks} relevant regulations out of {len(chunk_files)} chunks")
    
    # Use LLM to consolidate regulations
    try:
        consolidated_regulation = consolidator.consolidate_country_regulations(country, regulations)
        
        # Create final output structure
        output_data = {
            "metadata": {
                "consolidation_date": datetime.now().isoformat(),
                "country": country,
                "input_chunks_count": len(chunk_files),
                "relevant_regulations_count": valid_chunks,
                "consolidation_method": "LLM_COUNTRY_CONSOLIDATION",
                "description": f"Consolidated regulation for {country} from all relevant chunks"
            },
            "consolidated_regulation": consolidated_regulation,
            "source_statistics": {
                "total_chunks_processed": len(chunk_files),
                "relevant_chunks_used": valid_chunks,
                "unique_authorities": len(set(r.get('issuing_body', 'Unknown') for r in regulations if r.get('issuing_body') != 'Unknown')),
                "disclosure_areas": list(set(r.get('tag', 'Unknown') for r in regulations if r.get('tag') != 'Unknown')),
                "file_types_processed": list(set(r.get('document_metadata', {}).get('file_type', 'unknown') for r in regulations))
            }
        }
        
        return output_data
        
    except Exception as e:
        print(f"  Error consolidating {country}: {e}")
        return None


def save_consolidated_regulation(country: str, consolidated_data: Dict[str, Any], output_dir: str):
    """Save consolidated regulation to output directory"""
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # Create country-specific directory
    country_dir = output_path / country
    country_dir.mkdir(exist_ok=True)
    
    # Save consolidated regulation
    output_file = country_dir / f"{country}_consolidated_regulation.json"
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(consolidated_data, f, indent=2, ensure_ascii=False)
        
        print(f"  ðŸ’¾ Saved consolidated regulation: {output_file}")
        return output_file
        
    except Exception as e:
        print(f"  âŒ Error saving consolidated regulation for {country}: {e}")
        return None


def main():
    """Main consolidation function"""
    print("Chunk Consolidation Script")
    print("=" * 50)
    print("Reading all chunks from regulation_filter.py output")
    print("Creating consolidated JSON files by country")
    print()
    
    # Find all chunks organized by country
    print(f"ðŸ“‚ Scanning input directory: {INPUT_DIR}")
    country_chunks = find_all_chunks(INPUT_DIR)
    
    if not country_chunks:
        print("âŒ No chunk files found. Make sure regulation_filter.py has been run first.")
        return
    
    print(f"âœ… Found chunks for {len(country_chunks)} countries")
    
    # Initialize consolidator
    consolidator = ChunkConsolidator()
    
    # Process each country
    successful_consolidations = 0
    
    for country, chunk_files in country_chunks.items():
        print(f"\n{'='*60}")
        print(f"ðŸŒ Processing country: {country}")
        
        # Consolidate chunks for this country
        consolidated_data = consolidate_country_chunks(country, chunk_files, consolidator)
        
        if consolidated_data:
            # Save consolidated regulation
            output_file = save_consolidated_regulation(country, consolidated_data, OUTPUT_DIR)
            if output_file:
                successful_consolidations += 1
                
                # Print summary
                consolidated_reg = consolidated_data['consolidated_regulation']
                print(f"  âœ… Consolidated regulation created:")
                print(f"     Name: {consolidated_reg.get('regulation_name', 'Unknown')}")
                print(f"     Authorities: {consolidated_reg.get('issuing_body', 'Unknown')}")
                print(f"     Disclosure Areas: {consolidated_reg.get('tag', 'Unknown')}")
                print(f"     Effective Date: {consolidated_reg.get('effective_date', 'Unknown')}")
        else:
            print(f"  âŒ Failed to consolidate regulations for {country}")
    
    # Final summary
    print(f"\nðŸŽ‰ Consolidation completed!")
    print(f"ðŸ“Š Countries processed: {len(country_chunks)}")
    print(f"âœ… Successful consolidations: {successful_consolidations}")
    print(f"âŒ Failed consolidations: {len(country_chunks) - successful_consolidations}")
    print(f"ðŸ“ Output directory: {OUTPUT_DIR}")
    
    if successful_consolidations > 0:
        print(f"\nðŸ“‹ Consolidated regulation files saved in:")
        for country in country_chunks.keys():
            country_file = Path(OUTPUT_DIR) / country / f"{country}_consolidated_regulation.json"
            if country_file.exists():
                print(f"  â€¢ {country_file}")


if __name__ == "__main__":
    main()